{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPEELJ2sK382wZtOxdzddm7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreRra/ALEXANDRE_AMORIM_DDF_TECH_022025/blob/main/dadosfera_case.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "zlFYvcQ5BlyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch langdetect\n",
        "from google.colab import userdata\n",
        "from IPython.display import display\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import sqlite3\n",
        "import re\n",
        "import gc\n",
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import langdetect\n",
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HGgeixj96L_M",
        "outputId": "50381082-1195-4e91-adf8-347692bf3fe4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.61)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Settings\n"
      ],
      "metadata": {
        "id": "6ibF3CCp-uDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_lake_root = '/content/data_lake'\n",
        "os.makedirs(data_lake_root, exist_ok=True)\n",
        "\n",
        "database_root = f'{data_lake_root}/sqlite'\n",
        "os.makedirs(database_root, exist_ok=True)"
      ],
      "metadata": {
        "id": "g5R3x8Aa-wpY"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "SFUGqg4lTdzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_operation(message):\n",
        "    print(f\"[{datetime.datetime.now()}] {message}\")\n",
        "\n",
        "def get_db_connection():\n",
        "    return sqlite3.connect(f'{data_lake_root}/sqlite/db')"
      ],
      "metadata": {
        "id": "VAMEv_pDTdIl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Ingestion"
      ],
      "metadata": {
        "id": "oOyVo6tJBpM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "\n",
        "!kaggle datasets download -d piyushjain16/amazon-product-data\n",
        "\n",
        "file_path = '/content/dataset/train.csv'\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    !unzip -o \"amazon-product-data.zip\"\n",
        "else:\n",
        "    print(f\"File {file_path} already exists. Skipping unzip.\")\n",
        "\n",
        "dataset_csv_path = 'dataset/train.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SBpTLTut6u2I",
        "outputId": "8c2aeeda-8e6c-4065-bcc8-7d8bd486ae5c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/piyushjain16/amazon-product-data\n",
            "License(s): Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)\n",
            "amazon-product-data.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "File /content/dataset/train.csv already exists. Skipping unzip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Catalogs"
      ],
      "metadata": {
        "id": "hu9QzVKsk19D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Landing Zone"
      ],
      "metadata": {
        "id": "3PQi7AUC_TSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_operation(\"Starting Amazon products dataset loading\")\n",
        "\n",
        "landing_zone = f'{data_lake_root}/landing_zone'\n",
        "os.makedirs(landing_zone, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  # Read the file with options to handle possible encoding and delimiter issues\n",
        "  df = pd.read_csv(dataset_csv_path, encoding='utf-8', low_memory=False)\n",
        "  df = df.head(500000)  # Selects the first 500,000 rows\n",
        "\n",
        "  # Check basic information\n",
        "  log_operation(f\"Dataset loaded successfully. Dimensions: {df.shape}\")\n",
        "\n",
        "  # Display dataset information\n",
        "  print(\"\\nDataset Information:\")\n",
        "  print(f\"Number of records: {df.shape[0]}\")\n",
        "  print(f\"Number of columns: {df.shape[1]}\")\n",
        "  print(\"\\nColumns present in the dataset:\")\n",
        "  print(df.columns.tolist())\n",
        "\n",
        "  # Check for missing values\n",
        "  print(\"\\nCount of missing values per column:\")\n",
        "  print(df.isnull().sum())\n",
        "\n",
        "  # Count unique values per column\n",
        "  print(\"\\nUnique Values per Column:\")\n",
        "  print(df.nunique())\n",
        "\n",
        "  # Show first rows for inspection\n",
        "  print(\"\\nFirst 5 rows of the dataset:\")\n",
        "  print(df.head())\n",
        "\n",
        "  # Save the original file in the landing zone with a timestamp\n",
        "  timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "  landing_file = f\"{landing_zone}/amazon_products_raw_{timestamp}.parquet\"\n",
        "\n",
        "  # Save in parquet format for better compression and performance\n",
        "  df.to_parquet(landing_file, index=False)\n",
        "  log_operation(f\"Dataset saved in the Landing Zone: {landing_file}\")\n",
        "\n",
        "  # Register basic metadata\n",
        "  metadata = {\n",
        "      'source_file': file_path,\n",
        "      'landing_file': landing_file,\n",
        "      'rows': df.shape[0],\n",
        "      'columns': df.shape[1],\n",
        "      'column_names': df.columns.tolist(),\n",
        "      'timestamp': timestamp,\n",
        "      'null_counts': df.isnull().sum().to_dict()\n",
        "  }\n",
        "\n",
        "  # Save metadata\n",
        "  with open(f\"{landing_zone}/metadata_{timestamp}.json\", 'w') as f:\n",
        "      json.dump(metadata, f, indent=2)\n",
        "\n",
        "  log_operation(\"Loading process into the Landing Zone successfully completed\")\n",
        "  log_operation(\"Starting SQL Microtransformation\")\n",
        "\n",
        "  conn = get_db_connection()\n",
        "  cursor = conn.cursor()\n",
        "\n",
        "  cursor.execute('DROP TABLE IF EXISTS products')\n",
        "  cursor.execute('''\n",
        "      CREATE TABLE products (\n",
        "          PRODUCT_ID INTEGER PRIMARY KEY,\n",
        "          TITLE TEXT,\n",
        "          BULLET_POINTS TEXT,\n",
        "          DESCRIPTION TEXT,\n",
        "          PRODUCT_TYPE_ID INTEGER,\n",
        "          PRODUCT_LENGTH REAL\n",
        "      )\n",
        "  ''')\n",
        "  conn.commit()\n",
        "\n",
        "  df['PRODUCT_ID'] = df['PRODUCT_ID'].astype('int32')\n",
        "  df['PRODUCT_TYPE_ID'] = df['PRODUCT_TYPE_ID'].astype('int32')\n",
        "  df['PRODUCT_LENGTH'] = df['PRODUCT_LENGTH'].astype('float32')\n",
        "  df['TITLE'] = df['TITLE'].astype('category')\n",
        "  df['BULLET_POINTS'] = df['BULLET_POINTS'].astype('category')\n",
        "  df['DESCRIPTION'] = df['DESCRIPTION'].astype('category')\n",
        "\n",
        "  df.to_sql('products', conn, if_exists='replace', index=False)\n",
        "\n",
        "  # Fill missing DESCRIPTION values with BULLET_POINTS (filled_description)\n",
        "  query = '''\n",
        "  SELECT\n",
        "      PRODUCT_ID,\n",
        "      TITLE,\n",
        "      BULLET_POINTS,\n",
        "      DESCRIPTION,\n",
        "      PRODUCT_TYPE_ID,\n",
        "      PRODUCT_LENGTH,\n",
        "      COALESCE(DESCRIPTION, BULLET_POINTS) AS filled_description\n",
        "  FROM products\n",
        "  '''\n",
        "  df_transformed = pd.read_sql_query(query, conn)\n",
        "\n",
        "  print(\"\\nFirst 5 records after transformations:\")\n",
        "  print(df_transformed.head())\n",
        "\n",
        "  print(\"\\nCount of missing values per column:\")\n",
        "  print(df_transformed.isnull().sum())\n",
        "\n",
        "  conn.close()\n",
        "\n",
        "  del df, df_transformed\n",
        "  gc.collect()\n",
        "  log_operation(\"Microtransformation process successfully completed\")\n",
        "\n",
        "except Exception as e:\n",
        "    log_operation(f\"Error during dataset loading: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kveggRWz_UlJ",
        "outputId": "617e44f7-d162-438e-8183-9757e804024d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-02-27 19:39:17.756560] Starting Amazon products dataset loading\n",
            "[2025-02-27 19:39:41.305176] Dataset loaded successfully. Dimensions: (500000, 6)\n",
            "\n",
            "Dataset Information:\n",
            "Number of records: 500000\n",
            "Number of columns: 6\n",
            "\n",
            "Columns present in the dataset:\n",
            "['PRODUCT_ID', 'TITLE', 'BULLET_POINTS', 'DESCRIPTION', 'PRODUCT_TYPE_ID', 'PRODUCT_LENGTH']\n",
            "\n",
            "Count of missing values per column:\n",
            "PRODUCT_ID              0\n",
            "TITLE                   4\n",
            "BULLET_POINTS      185928\n",
            "DESCRIPTION        257106\n",
            "PRODUCT_TYPE_ID         0\n",
            "PRODUCT_LENGTH          0\n",
            "dtype: int64\n",
            "\n",
            "Unique Values per Column:\n",
            "PRODUCT_ID         500000\n",
            "TITLE              497028\n",
            "BULLET_POINTS      252406\n",
            "DESCRIPTION        194649\n",
            "PRODUCT_TYPE_ID      9793\n",
            "PRODUCT_LENGTH       8374\n",
            "dtype: int64\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "   PRODUCT_ID                                              TITLE  \\\n",
            "0     1925202  ArtzFolio Tulip Flowers Blackout Curtain for D...   \n",
            "1     2673191  Marks & Spencer Girls' Pyjama Sets T86_2561C_N...   \n",
            "2     2765088  PRIKNIK Horn Red Electric Air Horn Compressor ...   \n",
            "3     1594019  ALISHAH Women's Cotton Ankle Length Leggings C...   \n",
            "4      283658  The United Empire Loyalists: A Chronicle of th...   \n",
            "\n",
            "                                       BULLET_POINTS  \\\n",
            "0  [LUXURIOUS & APPEALING: Beautiful custom-made ...   \n",
            "1  [Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...   \n",
            "2  [Loud Dual Tone Trumpet Horn, Compatible With ...   \n",
            "3  [Made By 95%cotton and 5% Lycra which gives yo...   \n",
            "4                                                NaN   \n",
            "\n",
            "                                         DESCRIPTION  PRODUCT_TYPE_ID  \\\n",
            "0                                                NaN             1650   \n",
            "1                                                NaN             2755   \n",
            "2  Specifications: Color: Red, Material: Aluminiu...             7537   \n",
            "3  AISHAH Women's Lycra Cotton Ankel Leggings. Br...             2996   \n",
            "4                                                NaN             6112   \n",
            "\n",
            "   PRODUCT_LENGTH  \n",
            "0     2125.980000  \n",
            "1      393.700000  \n",
            "2      748.031495  \n",
            "3      787.401574  \n",
            "4      598.424000  \n",
            "[2025-02-27 19:39:52.975639] Dataset saved in the Landing Zone: /content/data_lake/landing_zone/amazon_products_raw_20250227_193942.parquet\n",
            "[2025-02-27 19:39:53.101085] Loading process into the Landing Zone successfully completed\n",
            "[2025-02-27 19:39:53.101186] Starting SQL Microtransformation\n",
            "\n",
            "First 5 records after transformations:\n",
            "   PRODUCT_ID                                              TITLE  \\\n",
            "0     1925202  ArtzFolio Tulip Flowers Blackout Curtain for D...   \n",
            "1     2673191  Marks & Spencer Girls' Pyjama Sets T86_2561C_N...   \n",
            "2     2765088  PRIKNIK Horn Red Electric Air Horn Compressor ...   \n",
            "3     1594019  ALISHAH Women's Cotton Ankle Length Leggings C...   \n",
            "4      283658  The United Empire Loyalists: A Chronicle of th...   \n",
            "\n",
            "                                       BULLET_POINTS  \\\n",
            "0  [LUXURIOUS & APPEALING: Beautiful custom-made ...   \n",
            "1  [Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...   \n",
            "2  [Loud Dual Tone Trumpet Horn, Compatible With ...   \n",
            "3  [Made By 95%cotton and 5% Lycra which gives yo...   \n",
            "4                                               None   \n",
            "\n",
            "                                         DESCRIPTION  PRODUCT_TYPE_ID  \\\n",
            "0                                               None             1650   \n",
            "1                                               None             2755   \n",
            "2  Specifications: Color: Red, Material: Aluminiu...             7537   \n",
            "3  AISHAH Women's Lycra Cotton Ankel Leggings. Br...             2996   \n",
            "4                                               None             6112   \n",
            "\n",
            "   PRODUCT_LENGTH                                 filled_description  \n",
            "0     2125.979980  [LUXURIOUS & APPEALING: Beautiful custom-made ...  \n",
            "1      393.700012  [Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...  \n",
            "2      748.031494  Specifications: Color: Red, Material: Aluminiu...  \n",
            "3      787.401550  AISHAH Women's Lycra Cotton Ankel Leggings. Br...  \n",
            "4      598.424011                                               None  \n",
            "\n",
            "Count of missing values per column:\n",
            "PRODUCT_ID                 0\n",
            "TITLE                      4\n",
            "BULLET_POINTS         185928\n",
            "DESCRIPTION           257106\n",
            "PRODUCT_TYPE_ID            0\n",
            "PRODUCT_LENGTH             0\n",
            "filled_description    174027\n",
            "dtype: int64\n",
            "[2025-02-27 19:44:04.094506] Microtransformation process successfully completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardized Zone"
      ],
      "metadata": {
        "id": "9p9QLDRAk7KF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "standardized_zone = f'{data_lake_root}/standardized_zone'\n",
        "os.makedirs(standardized_zone, exist_ok=True)\n",
        "\n",
        "try:\n",
        "\n",
        "  log_operation(\"Searching for the most recent file in the Landing Zone\")\n",
        "\n",
        "  landing_files = [f for f in os.listdir(landing_zone) if f.endswith('.parquet')]\n",
        "  latest_file = sorted(landing_files)[-1]\n",
        "  landing_file_path = f\"{landing_zone}/{latest_file}\"\n",
        "\n",
        "  log_operation(f\"Raw file found: {landing_file_path}\")\n",
        "\n",
        "  # Load the dataset\n",
        "  df = pd.read_parquet(landing_file_path)\n",
        "  log_operation(f\"Dataset loaded successfully. Dimensions: {df.shape}\")\n",
        "\n",
        "  log_operation(\"Standardizing column names\")\n",
        "  df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "  log_operation(\"Starting data cleaning and standardization\")\n",
        "\n",
        "  # Function to remove special characters and HTML\n",
        "  def clean_text(text):\n",
        "      if pd.isna(text):\n",
        "          return text\n",
        "\n",
        "      # Convert to string\n",
        "      text = str(text)\n",
        "\n",
        "      # Remove HTML tags\n",
        "      text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "      # Remove special characters and keep only alphanumeric, spaces, and basic punctuation\n",
        "      text = re.sub(r'[^\\w\\s.,;:!?-]', ' ', text)\n",
        "\n",
        "      # Remove multiple spaces\n",
        "      text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "      return text.strip()\n",
        "\n",
        "  # Apply cleaning to text columns\n",
        "  text_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
        "  for col in text_columns:\n",
        "      log_operation(f\"Cleaning text column: {col}\")\n",
        "      df[col] = df[col].apply(clean_text)\n",
        "\n",
        "  # 3. Handling missing values\n",
        "  log_operation(\"Handling missing values\")\n",
        "\n",
        "  # For text columns, replace NaN with an empty string\n",
        "  for col in text_columns:\n",
        "      df[col] = df[col].fillna('')\n",
        "\n",
        "  # For numeric columns, replace NaN with 0 or mean, depending on the context\n",
        "  numeric_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "  for col in numeric_columns:\n",
        "      df[col] = df[col].fillna(0)\n",
        "\n",
        "  # 4. Extract features from description and bullet points\n",
        "  log_operation(\"Extracting features from text\")\n",
        "\n",
        "  # Check if there is a description column\n",
        "  if 'description' in df.columns:\n",
        "      # Create feature for description length\n",
        "      df['description_length'] = df['description'].apply(lambda x: len(str(x)))\n",
        "\n",
        "      # Create feature for counting keywords\n",
        "      keywords = ['quality', 'premium', 'best', 'new', 'improved']\n",
        "      df['keyword_count'] = df['description'].apply(\n",
        "          lambda x: sum(1 for keyword in keywords if keyword.lower() in str(x).lower())\n",
        "      )\n",
        "\n",
        "  # Check if there is a bullet points column\n",
        "  if 'bullet_points' in df.columns:\n",
        "      # Count number of bullet points\n",
        "      df['bullet_count'] = df['bullet_points'].apply(\n",
        "          lambda x: len(str(x).split('\\n')) if pd.notna(x) else 0\n",
        "      )\n",
        "\n",
        "  # 6. Encoding categorical variables\n",
        "  log_operation(\"Encoding categorical variables\")\n",
        "\n",
        "  # Identify categorical columns (with few unique values)\n",
        "  categorical_columns = []\n",
        "  for col in text_columns:\n",
        "      if df[col].nunique() < 50:  # Arbitrary limit, adjust as needed\n",
        "          categorical_columns.append(col)\n",
        "\n",
        "  # Encode categorical variables\n",
        "  label_encoders = {}\n",
        "  for col in categorical_columns:\n",
        "      le = LabelEncoder()\n",
        "      not_null = df[col].notna()\n",
        "      if not_null.any():\n",
        "          df.loc[not_null, f'{col}_encoded'] = le.fit_transform(df.loc[not_null, col])\n",
        "          label_encoders[col] = {label: idx for idx, label in enumerate(le.classes_)}\n",
        "      else:\n",
        "          df[f'{col}_encoded'] = np.nan\n",
        "\n",
        "  # 7. Save the standardized dataset\n",
        "  timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "  standardized_file = f\"{standardized_zone}/amazon_products_standardized_{timestamp}.parquet\"\n",
        "  df.to_parquet(standardized_file, index=False)\n",
        "\n",
        "  log_operation(f\"Standardized dataset saved at: {standardized_file}\")\n",
        "\n",
        "  # Create data dictionary\n",
        "  log_operation(\"Generating data dictionary\")\n",
        "\n",
        "  # Basic structure for the data dictionary\n",
        "  data_dict = []\n",
        "  for col in df.columns:\n",
        "      col_info = {\n",
        "          'column_name': col,\n",
        "          'data_type': str(df[col].dtype),\n",
        "          'description': '',\n",
        "          'sample_values': str(df[col].head(3).tolist()),\n",
        "          'null_count': int(df[col].isna().sum()),\n",
        "          'unique_values': int(df[col].nunique()),\n",
        "      }\n",
        "\n",
        "      if 'product_id' in col:\n",
        "          col_info['description'] = 'Unique ID for each product'\n",
        "      elif 'title' in col:\n",
        "          col_info['description'] = 'Name of the product'\n",
        "      elif 'bullet_points' in col:\n",
        "          col_info['description'] = 'Main features of the product in list format'\n",
        "      elif 'description' == col:\n",
        "          col_info['description'] = 'Full description of the product'\n",
        "      elif 'product_length' in col:\n",
        "          col_info['description'] = 'Physical dimensions of the product'\n",
        "\n",
        "      data_dict.append(col_info)\n",
        "\n",
        "  # Save the data dictionary\n",
        "  import json\n",
        "  with open(f\"{standardized_zone}/data_dictionary_{timestamp}.json\", 'w') as f:\n",
        "      json.dump(data_dict, f, indent=2)\n",
        "\n",
        "  log_operation(\"Standardization process completed successfully\")\n",
        "\n",
        "  # Display information about the standardized dataset\n",
        "  print(\"\\nStandardized Dataset Information:\")\n",
        "  print(f\"Number of records: {df.shape[0]}\")\n",
        "  print(f\"Number of columns: {df.shape[1]}\")\n",
        "  print(\"\\nNew columns created:\")\n",
        "  original_cols = set(pd.read_parquet(landing_file_path).columns)\n",
        "  new_cols = set(df.columns) - original_cols\n",
        "  print(list(new_cols))\n",
        "except Exception as e:\n",
        "    log_operation(f\"Error during creation of the Standardized Zone: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbBiEtIRk6yP",
        "outputId": "84d67763-5588-41f4-aa47-57ce47956064"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-02-27 19:44:04.142824] Searching for the most recent file in the Landing Zone\n",
            "[2025-02-27 19:44:04.144099] Raw file found: /content/data_lake/landing_zone/amazon_products_raw_20250227_193942.parquet\n",
            "[2025-02-27 19:44:07.441312] Dataset loaded successfully. Dimensions: (500000, 6)\n",
            "[2025-02-27 19:44:07.441415] Standardizing column names\n",
            "[2025-02-27 19:44:07.442530] Starting data cleaning and standardization\n",
            "[2025-02-27 19:44:07.479483] Cleaning text column: title\n",
            "[2025-02-27 19:44:10.816699] Cleaning text column: bullet_points\n",
            "[2025-02-27 19:44:21.673204] Cleaning text column: description\n",
            "[2025-02-27 19:44:31.467775] Handling missing values\n",
            "[2025-02-27 19:44:31.714744] Extracting features from text\n",
            "[2025-02-27 19:44:33.735061] Encoding categorical variables\n",
            "[2025-02-27 19:44:41.340169] Standardized dataset saved at: /content/data_lake/standardized_zone/amazon_products_standardized_20250227_194434.parquet\n",
            "[2025-02-27 19:44:41.340312] Generating data dictionary\n",
            "[2025-02-27 19:44:42.653229] Standardization process completed successfully\n",
            "\n",
            "Standardized Dataset Information:\n",
            "Number of records: 500000\n",
            "Number of columns: 9\n",
            "\n",
            "New columns created:\n",
            "['description_length', 'bullet_points', 'product_length', 'bullet_count', 'product_type_id', 'product_id', 'title', 'description', 'keyword_count']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Staging Zone"
      ],
      "metadata": {
        "id": "kbXnPn4mmGzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "staging_zone = f'{data_lake_root}/staging_zone'\n",
        "\n",
        "# Find the most recent file in the standardized zone\n",
        "log_operation(\"Finding the most recent file in the Standardized Zone\")\n",
        "\n",
        "standardized_files = [f for f in os.listdir(standardized_zone) if f.endswith('.parquet')]\n",
        "latest_file = sorted(standardized_files)[-1]\n",
        "standardized_file_path = f\"{standardized_zone}/{latest_file}\"\n",
        "\n",
        "log_operation(f\"File found: {standardized_file_path}\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_parquet(standardized_file_path)\n",
        "log_operation(f\"Dataset loaded successfully. Dimensions: {df.shape}\")\n",
        "\n",
        "try:\n",
        "  # 1. Create a column that tracks which columns are null\n",
        "  log_operation(\"Creating null tracking column\")\n",
        "\n",
        "  # Check for 'title', 'bullet_points', and 'description' columns\n",
        "  # If they don't exist but similar columns do, map them\n",
        "  column_mapping = {}\n",
        "\n",
        "  # Check for title column\n",
        "  title_candidates = ['title']\n",
        "  for col in title_candidates:\n",
        "      if col in df.columns:\n",
        "          column_mapping['title'] = col\n",
        "          break\n",
        "\n",
        "  # Check for bullet_points column\n",
        "  bullet_candidates = ['bullet_points']\n",
        "  for col in bullet_candidates:\n",
        "      if col in df.columns:\n",
        "          column_mapping['bullet_points'] = col\n",
        "          break\n",
        "\n",
        "  # Check for description column\n",
        "  desc_candidates = ['description']\n",
        "  for col in desc_candidates:\n",
        "      if col in df.columns:\n",
        "          column_mapping['description'] = col\n",
        "          break\n",
        "\n",
        "  log_operation(f\"Column mapping established: {column_mapping}\")\n",
        "\n",
        "  # Create working copies with standardized column names\n",
        "  working_df = df.copy()\n",
        "\n",
        "  # Create standardized column names in working dataframe\n",
        "  for std_col, actual_col in column_mapping.items():\n",
        "      if std_col != actual_col:\n",
        "          working_df[std_col] = df[actual_col]\n",
        "\n",
        "  # Ensure all three columns exist, create them if they don't\n",
        "  for col in ['title', 'bullet_points', 'description']:\n",
        "      if col not in working_df.columns:\n",
        "          log_operation(f\"Column {col} not found, creating empty column\")\n",
        "          working_df[col] = np.nan\n",
        "\n",
        "  # Create the null tracking column\n",
        "  working_df['null_columns'] = ''\n",
        "\n",
        "  # Check which columns are null and update the null_columns\n",
        "  for col in ['title', 'bullet_points', 'description']:\n",
        "      # Add column name to null_columns if the value is null\n",
        "      working_df.loc[working_df[col].isna(), 'null_columns'] = working_df.loc[working_df[col].isna(), 'null_columns'] + col + ','\n",
        "\n",
        "  # Remove trailing comma\n",
        "  working_df['null_columns'] = working_df['null_columns'].str.rstrip(',')\n",
        "\n",
        "  # 2. Copy content between columns when values are missing\n",
        "  log_operation(\"Copying content between columns to fill missing values\")\n",
        "\n",
        "  # Copy bullet_points to description and vice versa when one is missing\n",
        "  # First, convert all to string to avoid type issues\n",
        "  for col in ['title', 'bullet_points', 'description']:\n",
        "      working_df[col] = working_df[col].astype(str).replace('nan', '')\n",
        "\n",
        "  # Copy bullet_points to description when description is empty\n",
        "  working_df.loc[working_df['description'] == '', 'description'] = working_df.loc[working_df['description'] == '', 'bullet_points']\n",
        "\n",
        "  # Copy description to bullet_points when bullet_points is empty\n",
        "  working_df.loc[working_df['bullet_points'] == '', 'bullet_points'] = working_df.loc[working_df['bullet_points'] == '', 'description']\n",
        "\n",
        "  # If both description and bullet_points are empty, copy title to both\n",
        "  working_df.loc[(working_df['description'] == '') & (working_df['bullet_points'] == ''), 'description'] = \\\n",
        "      working_df.loc[(working_df['description'] == '') & (working_df['bullet_points'] == ''), 'title']\n",
        "\n",
        "  working_df.loc[(working_df['description'] == '') & (working_df['bullet_points'] == ''), 'bullet_points'] = \\\n",
        "      working_df.loc[(working_df['description'] == '') & (working_df['bullet_points'] == ''), 'title']\n",
        "\n",
        "  # 3. Detect language of the title\n",
        "  log_operation(\"Detecting language of the title\")\n",
        "\n",
        "  # Function to detect language with error handling\n",
        "  def detect_language(text):\n",
        "      if not isinstance(text, str) or text.strip() == '':\n",
        "          return 'unknown'\n",
        "\n",
        "      try:\n",
        "          return detect(text)\n",
        "      except LangDetectException:\n",
        "          return 'unknown'\n",
        "      except Exception as e:\n",
        "          log_operation(f\"Error detecting language: {str(e)}\")\n",
        "          return 'unknown'\n",
        "\n",
        "  # Apply language detection to title column\n",
        "  log_operation(\"This might take a while for large datasets...\")\n",
        "  working_df['title_language'] = working_df['title'].apply(detect_language)\n",
        "\n",
        "  # 4. Calculate statistics on the enhancements\n",
        "  log_operation(\"Calculating statistics on enhancements\")\n",
        "\n",
        "  # Count how many rows had content copied between columns\n",
        "  description_filled = (working_df['description'] != '') & (working_df['null_columns'].str.contains('description'))\n",
        "  bullet_points_filled = (working_df['bullet_points'] != '') & (working_df['null_columns'].str.contains('bullet_points'))\n",
        "\n",
        "  stats = {\n",
        "      'rows_processed': len(working_df),\n",
        "      'rows_with_null_columns': (working_df['null_columns'] != '').sum(),\n",
        "      'description_filled_from_other_columns': int(description_filled.sum()),\n",
        "      'bullet_points_filled_from_other_columns': int(bullet_points_filled.sum()),\n",
        "      'language_distribution': working_df['title_language'].value_counts().to_dict()\n",
        "  }\n",
        "\n",
        "  log_operation(\"Enhancement statistics:\")\n",
        "  for key, value in stats.items():\n",
        "      if key != 'language_distribution':\n",
        "          log_operation(f\"- {key}: {value}\")\n",
        "\n",
        "  log_operation(\"Top 5 detected languages:\")\n",
        "  for lang, count in sorted(stats['language_distribution'].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "      log_operation(f\"- {lang}: {count}\")\n",
        "\n",
        "  # 5. Save the enhanced dataset\n",
        "  timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "  staging_file = f\"{staging_zone}/amazon_products_staging_{timestamp}.parquet\"\n",
        "  working_df.to_parquet(staging_file, index=False)\n",
        "\n",
        "  log_operation(f\"Enhanced dataset saved to: {staging_file}\")\n",
        "\n",
        "  # 6. Save enhancement metadata and statistics\n",
        "  metadata = {\n",
        "      'source_file': standardized_file_path,\n",
        "      'staging_file': staging_file,\n",
        "      'timestamp': timestamp,\n",
        "      'enhancements': [\n",
        "          'null_columns_tracking',\n",
        "          'cross_column_content_copying',\n",
        "          'language_detection'\n",
        "      ],\n",
        "      'stats': stats,\n",
        "      'column_mapping': column_mapping\n",
        "  }\n",
        "\n",
        "  with open(f\"{staging_zone}/staging_metadata_{timestamp}.json\", 'w') as f:\n",
        "      json.dump(metadata, f, indent=2)\n",
        "\n",
        "  log_operation(\"Enhancement process completed successfully\")\n",
        "\n",
        "  # Display sample rows to verify the enhancements\n",
        "  print(\"\\nSample rows from enhanced dataset:\")\n",
        "  display_cols = ['title', 'bullet_points', 'description', 'null_columns', 'title_language']\n",
        "  print(working_df[display_cols].head().to_string())\n",
        "\n",
        "  staging_rows_pct = ((description_filled | bullet_points_filled).sum() / len(working_df)) * 100\n",
        "  print(f\"\\nPercentage of rows that received content enhancements: {staging_rows_pct:.2f}%\")\n",
        "except Exception as e:\n",
        "  log_operation(f\"Error during creation of the Staging Zone: {str(e)}\")"
      ],
      "metadata": {
        "id": "y0F1YenAry8D",
        "outputId": "bdd07121-801d-449d-98f5-2fced991445c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-02-27 19:49:31.142592] Finding the most recent file in the Standardized Zone\n",
            "[2025-02-27 19:49:31.143507] File found: /content/data_lake/standardized_zone/amazon_products_standardized_20250227_194434.parquet\n",
            "[2025-02-27 19:49:34.892786] Dataset loaded successfully. Dimensions: (500000, 9)\n",
            "[2025-02-27 19:49:34.893809] Creating null tracking column\n",
            "[2025-02-27 19:49:34.913786] Column mapping established: {'title': 'title', 'bullet_points': 'bullet_points', 'description': 'description'}\n",
            "[2025-02-27 19:49:35.862136] Copying content between columns to fill missing values\n",
            "[2025-02-27 19:49:36.947905] Detecting language of the title\n",
            "[2025-02-27 19:49:36.948052] This might take a while for large datasets...\n",
            "[2025-02-27 20:38:04.205707] Calculating statistics on enhancements\n",
            "[2025-02-27 20:38:04.668248] Enhancement statistics:\n",
            "[2025-02-27 20:38:04.668372] - rows_processed: 500000\n",
            "[2025-02-27 20:38:04.668402] - rows_with_null_columns: 0\n",
            "[2025-02-27 20:38:04.668423] - description_filled_from_other_columns: 0\n",
            "[2025-02-27 20:38:04.668442] - bullet_points_filled_from_other_columns: 0\n",
            "[2025-02-27 20:38:04.668461] Top 5 detected languages:\n",
            "[2025-02-27 20:38:04.668501] - en: 425297\n",
            "[2025-02-27 20:38:04.668523] - de: 26157\n",
            "[2025-02-27 20:38:04.668542] - fr: 13558\n",
            "[2025-02-27 20:38:04.668560] - es: 4626\n",
            "[2025-02-27 20:38:04.668578] - it: 4364\n",
            "[2025-02-27 20:38:06.758716] Error during creation of the Staging Zone: Cannot save file into a non-existent directory: '/content/data_lake/staging_zone'\n"
          ]
        }
      ]
    }
  ]
}
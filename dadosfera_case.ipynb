{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPURFXqIa2XOHlPbtowqZ6I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreRra/ALEXANDRE_AMORIM_DDF_TECH_022025/blob/main/dadosfera_case.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "zlFYvcQ5BlyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch\n",
        "from google.colab import userdata\n",
        "from IPython.display import display\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import sqlite3\n",
        "import re\n",
        "import gc\n",
        "import os\n",
        "import zipfile\n",
        "import json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HGgeixj96L_M",
        "outputId": "dc8bd9bf-9f13-4b90-c8ee-3f268b2579e6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.61)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Settings\n"
      ],
      "metadata": {
        "id": "6ibF3CCp-uDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_lake_root = '/content/data_lake'\n",
        "landing_zone = f'{data_lake_root}/landing_zone'\n",
        "\n",
        "os.makedirs(landing_zone, exist_ok=True)\n",
        "\n",
        "def log_operation(message):\n",
        "    print(f\"[{datetime.datetime.now()}] {message}\")\n",
        "\n",
        "database_root = f'{data_lake_root}/sqlite'\n",
        "os.makedirs(database_root, exist_ok=True)"
      ],
      "metadata": {
        "id": "g5R3x8Aa-wpY"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Ingestion"
      ],
      "metadata": {
        "id": "oOyVo6tJBpM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "\n",
        "!kaggle datasets download -d piyushjain16/amazon-product-data\n",
        "\n",
        "file_path = '/content/dataset/train.csv'\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    !unzip -o \"amazon-product-data.zip\"\n",
        "else:\n",
        "    print(f\"File {file_path} already exists. Skipping unzip.\")\n",
        "\n",
        "dataset_csv_path = 'dataset/train.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SBpTLTut6u2I",
        "outputId": "b78feb55-ddfa-4dfa-c2cb-82a6cfd1fe1b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/piyushjain16/amazon-product-data\n",
            "License(s): Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)\n",
            "amazon-product-data.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "File /content/dataset/train.csv already exists. Skipping unzip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('dataset/train.csv')\n",
        "\n",
        "# # Visualizar as primeiras linhas para confirmar o carregamento\n",
        "# print(\"Primeiras 5 linhas do dataset:\")\n",
        "# display(df.head())\n",
        "\n",
        "# ## 3. Análise Descritiva Básica\n",
        "# # Informações gerais do dataset (colunas, tipos, valores nulos)\n",
        "# print(\"\\nInformações do Dataset:\")\n",
        "# display(df.info())\n",
        "\n",
        "# # Estatísticas descritivas para colunas numéricas\n",
        "# print(\"\\nEstatísticas Descritivas:\")\n",
        "# display(df.describe())\n",
        "\n",
        "# # Contagem de valores únicos por coluna\n",
        "# print(\"\\nValores Únicos por Coluna:\")\n",
        "# display(df.nunique())\n",
        "\n",
        "# # Verificação de valores nulos\n",
        "# print(\"\\nValores Nulos por Coluna:\")\n",
        "# display(df.isnull().sum())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "E2tyK8tVZnUO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_db_connection():\n",
        "    return sqlite3.connect(f'{data_lake_root}/sqlite/db')\n",
        "\n",
        "conn = get_db_connection()\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('DROP TABLE IF EXISTS products')\n",
        "cursor.execute('''\n",
        "    CREATE TABLE products (\n",
        "        PRODUCT_ID INTEGER PRIMARY KEY,\n",
        "        TITLE TEXT,\n",
        "        BULLET_POINTS TEXT,\n",
        "        DESCRIPTION TEXT,\n",
        "        PRODUCT_TYPE_ID INTEGER,\n",
        "        PRODUCT_LENGTH REAL\n",
        "    )\n",
        "''')\n",
        "conn.commit()\n",
        "\n",
        "df = pd.read_csv(dataset_csv_path)\n",
        "\n",
        "df['PRODUCT_ID'] = df['PRODUCT_ID'].astype('int32')\n",
        "df['PRODUCT_TYPE_ID'] = df['PRODUCT_TYPE_ID'].astype('int32')\n",
        "df['PRODUCT_LENGTH'] = df['PRODUCT_LENGTH'].astype('float32')\n",
        "df['TITLE'] = df['TITLE'].astype('category')\n",
        "df['BULLET_POINTS'] = df['BULLET_POINTS'].astype('category')\n",
        "df['DESCRIPTION'] = df['DESCRIPTION'].astype('category')\n",
        "\n",
        "df.to_sql('products', conn, if_exists='replace', index=False)\n",
        "\n",
        "# Preencher DESCRIPTION nula com BULLET_POINTS (filled_description)\n",
        "query = '''\n",
        "SELECT\n",
        "    PRODUCT_ID,\n",
        "    TITLE,\n",
        "    BULLET_POINTS,\n",
        "    DESCRIPTION,\n",
        "    PRODUCT_TYPE_ID,\n",
        "    PRODUCT_LENGTH,\n",
        "    COALESCE(DESCRIPTION, BULLET_POINTS) AS filled_description\n",
        "FROM products\n",
        "'''\n",
        "df_transformed = pd.read_sql_query(query, conn)\n",
        "\n",
        "print(\"\\nPrimeiros 5 registros após transformações:\")\n",
        "print(df_transformed.head())\n",
        "\n",
        "conn.close()\n",
        "\n",
        "del df, df_transformed\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "5bbPxpoAbq6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Catálogos"
      ],
      "metadata": {
        "id": "hu9QzVKsk19D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Landing Zone"
      ],
      "metadata": {
        "id": "3PQi7AUC_TSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_operation(\"Iniciando carga do dataset de produtos Amazon\")\n",
        "\n",
        "try:\n",
        "    # Ler o arquivo com opções para lidar com possíveis problemas de encoding e delimitadores\n",
        "    df = pd.read_csv(dataset_csv_path, encoding='utf-8', low_memory=False)\n",
        "\n",
        "    # Verificar informações básicas\n",
        "    log_operation(f\"Dataset carregado com sucesso. Dimensões: {df.shape}\")\n",
        "\n",
        "    # Mostrar informações do dataset\n",
        "    print(\"\\nInformações do Dataset:\")\n",
        "    print(f\"Número de registros: {df.shape[0]}\")\n",
        "    print(f\"Número de colunas: {df.shape[1]}\")\n",
        "    print(\"\\nColunas presentes no dataset:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    # Verificar valores nulos\n",
        "    print(\"\\nContagem de valores nulos por coluna:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    # Mostrar primeiras linhas para inspeção\n",
        "    print(\"\\nPrimeiras 5 linhas do dataset:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Salvar o arquivo original na landing zone com timestamp\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    landing_file = f\"{landing_zone}/amazon_products_raw_{timestamp}.parquet\"\n",
        "\n",
        "    # Salvar em formato parquet para melhor compressão e performance\n",
        "    df.to_parquet(landing_file, index=False)\n",
        "    log_operation(f\"Dataset salvo na Landing Zone: {landing_file}\")\n",
        "\n",
        "    # Registrar metadados básicos\n",
        "    metadata = {\n",
        "        'source_file': file_path,\n",
        "        'landing_file': landing_file,\n",
        "        'rows': df.shape[0],\n",
        "        'columns': df.shape[1],\n",
        "        'column_names': df.columns.tolist(),\n",
        "        'timestamp': timestamp,\n",
        "        'null_counts': df.isnull().sum().to_dict()\n",
        "    }\n",
        "\n",
        "    # Salvar metadados\n",
        "    with open(f\"{landing_zone}/metadata_{timestamp}.json\", 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    log_operation(\"Processo de carga na Landing Zone concluído com sucesso\")\n",
        "\n",
        "except Exception as e:\n",
        "    log_operation(f\"Erro durante o carregamento do dataset: {str(e)}\")"
      ],
      "metadata": {
        "id": "kveggRWz_UlJ",
        "outputId": "fe3ef15c-6ea8-47d2-ec87-9b969bb6e55d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-02-27 15:59:42.834233] Iniciando carga do dataset de produtos Amazon\n",
            "[2025-02-27 15:59:59.915907] Dataset carregado com sucesso. Dimensões: (2249698, 6)\n",
            "\n",
            "Informações do Dataset:\n",
            "Número de registros: 2249698\n",
            "Número de colunas: 6\n",
            "\n",
            "Colunas presentes no dataset:\n",
            "['PRODUCT_ID', 'TITLE', 'BULLET_POINTS', 'DESCRIPTION', 'PRODUCT_TYPE_ID', 'PRODUCT_LENGTH']\n",
            "\n",
            "Contagem de valores nulos por coluna:\n",
            "PRODUCT_ID               0\n",
            "TITLE                   13\n",
            "BULLET_POINTS       837366\n",
            "DESCRIPTION        1157382\n",
            "PRODUCT_TYPE_ID          0\n",
            "PRODUCT_LENGTH           0\n",
            "dtype: int64\n",
            "\n",
            "Primeiras 5 linhas do dataset:\n",
            "   PRODUCT_ID                                              TITLE  \\\n",
            "0     1925202  ArtzFolio Tulip Flowers Blackout Curtain for D...   \n",
            "1     2673191  Marks & Spencer Girls' Pyjama Sets T86_2561C_N...   \n",
            "2     2765088  PRIKNIK Horn Red Electric Air Horn Compressor ...   \n",
            "3     1594019  ALISHAH Women's Cotton Ankle Length Leggings C...   \n",
            "4      283658  The United Empire Loyalists: A Chronicle of th...   \n",
            "\n",
            "                                       BULLET_POINTS  \\\n",
            "0  [LUXURIOUS & APPEALING: Beautiful custom-made ...   \n",
            "1  [Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...   \n",
            "2  [Loud Dual Tone Trumpet Horn, Compatible With ...   \n",
            "3  [Made By 95%cotton and 5% Lycra which gives yo...   \n",
            "4                                                NaN   \n",
            "\n",
            "                                         DESCRIPTION  PRODUCT_TYPE_ID  \\\n",
            "0                                                NaN             1650   \n",
            "1                                                NaN             2755   \n",
            "2  Specifications: Color: Red, Material: Aluminiu...             7537   \n",
            "3  AISHAH Women's Lycra Cotton Ankel Leggings. Br...             2996   \n",
            "4                                                NaN             6112   \n",
            "\n",
            "   PRODUCT_LENGTH  \n",
            "0     2125.980000  \n",
            "1      393.700000  \n",
            "2      748.031495  \n",
            "3      787.401574  \n",
            "4      598.424000  \n",
            "[2025-02-27 16:00:35.922339] Dataset salvo na Landing Zone: /content/data_lake/landing_zone/amazon_products_raw_20250227_160000.parquet\n",
            "[2025-02-27 16:00:36.441081] Processo de carga na Landing Zone concluído com sucesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "092VaBh5TDP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Landing Zone"
      ],
      "metadata": {
        "id": "9p9QLDRAk7KF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook 3: Standardized Zone - Padronização e Limpeza dos Dados\n",
        "# Objetivo: Limpar, padronizar e normalizar os dados para análise\n",
        "\n",
        "# Montar o Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Definir diretórios de trabalho no Data Lake\n",
        "# data_lake_root = '/content/drive/MyDrive/data_lake_amazon'\n",
        "# masked_zone = f'{data_lake_root}/masked_zone'\n",
        "# standardized_zone = f'{data_lake_root}/standardized_zone'\n",
        "\n",
        "# # Criar diretórios se não existirem\n",
        "# os.makedirs(standardized_zone, exist_ok=True)\n",
        "\n",
        "# # Função para registrar logs\n",
        "# def log_operation(message):\n",
        "#     print(f\"[{datetime.datetime.now()}] {message}\")\n",
        "\n",
        "# # Buscar o arquivo mais recente na masked zone\n",
        "# log_operation(\"Buscando o arquivo mais recente na Masked Zone\")\n",
        "\n",
        "# masked_files = [f for f in os.listdir(masked_zone) if f.endswith('.parquet')]\n",
        "# latest_file = sorted(masked_files)[-1]  # Pega o arquivo mais recente\n",
        "# masked_file_path = f\"{masked_zone}/{latest_file}\"\n",
        "\n",
        "def log_operation(message):\n",
        "    print(f\"[{datetime.datetime.now()}] {message}\")\n",
        "\n",
        "# Carregar o dataset\n",
        "df = pd.read_csv('dataset/train.csv')\n",
        "log_operation(f\"Dataset carregado com sucesso. Dimensões: {df.shape}\")\n",
        "\n",
        "# 1. Padronização dos nomes das colunas\n",
        "log_operation(\"Padronizando nomes das colunas\")\n",
        "df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# 2. Limpeza e padronização dos dados\n",
        "log_operation(\"Iniciando limpeza e padronização dos dados\")\n",
        "\n",
        "# Função para remover caracteres especiais e HTML\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "\n",
        "    # Converter para string\n",
        "    text = str(text)\n",
        "\n",
        "    # Remover tags HTML\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remover caracteres especiais e manter apenas alfanuméricos, espaços e pontuação básica\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?-]', ' ', text)\n",
        "\n",
        "    # Remover múltiplos espaços\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Aplicar limpeza às colunas de texto\n",
        "text_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
        "for col in text_columns:\n",
        "    log_operation(f\"Limpando coluna de texto: {col}\")\n",
        "    df[col] = df[col].apply(clean_text)\n",
        "\n",
        "# 3. Tratamento de valores nulos\n",
        "log_operation(\"Tratando valores nulos\")\n",
        "\n",
        "# Para colunas de texto, substituir NaN por string vazia\n",
        "for col in text_columns:\n",
        "    df[col] = df[col].fillna('')\n",
        "\n",
        "# Para colunas numéricas, substituir NaN por 0 ou média, dependendo do contexto\n",
        "numeric_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "for col in numeric_columns:\n",
        "    df[col] = df[col].fillna(0)\n",
        "\n",
        "# 4. Extração de features da descrição e bullet points\n",
        "log_operation(\"Extraindo features de texto\")\n",
        "\n",
        "# Verificar se existe coluna de descrição\n",
        "if 'description' in df.columns:\n",
        "    # Criar feature de comprimento da descrição\n",
        "    df['description_length'] = df['description'].apply(lambda x: len(str(x)))\n",
        "\n",
        "    # Criar feature de contagem de palavras-chave\n",
        "    keywords = ['quality', 'premium', 'best', 'new', 'improved']\n",
        "    df['keyword_count'] = df['description'].apply(\n",
        "        lambda x: sum(1 for keyword in keywords if keyword.lower() in str(x).lower())\n",
        "    )\n",
        "\n",
        "# Verificar se existe coluna de bullet points\n",
        "if 'bullet_points' in df.columns:\n",
        "    # Contar número de bullet points\n",
        "    df['bullet_count'] = df['bullet_points'].apply(\n",
        "        lambda x: len(str(x).split('\\n')) if pd.notna(x) else 0\n",
        "    )\n",
        "\n",
        "# 6. Codificação de variáveis categóricas\n",
        "log_operation(\"Codificando variáveis categóricas\")\n",
        "\n",
        "# Identificar colunas categóricas (com poucos valores únicos)\n",
        "categorical_columns = []\n",
        "for col in text_columns:\n",
        "    if df[col].nunique() < 50:  # Limite arbitrário, ajustar conforme necessário\n",
        "        categorical_columns.append(col)\n",
        "\n",
        "# Codificar variáveis categóricas\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    not_null = df[col].notna()\n",
        "    if not_null.any():\n",
        "        df.loc[not_null, f'{col}_encoded'] = le.fit_transform(df.loc[not_null, col])\n",
        "        label_encoders[col] = {label: idx for idx, label in enumerate(le.classes_)}\n",
        "    else:\n",
        "        df[f'{col}_encoded'] = np.nan\n",
        "\n",
        "# 7. Salvar o dataset padronizado\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "standardized_file = f\"{standardized_zone}/amazon_products_standardized_{timestamp}.parquet\"\n",
        "df.to_parquet(standardized_file, index=False)\n",
        "\n",
        "log_operation(f\"Dataset padronizado salvo em: {standardized_file}\")\n",
        "\n",
        "# Criação do dicionário de dados\n",
        "log_operation(\"Gerando dicionário de dados\")\n",
        "\n",
        "# Estrutura básica para o dicionário de dados\n",
        "data_dict = []\n",
        "for col in df.columns:\n",
        "    col_info = {\n",
        "        'column_name': col,\n",
        "        'data_type': str(df[col].dtype),\n",
        "        'description': '',  # A ser preenchido manualmente\n",
        "        'sample_values': str(df[col].head(3).tolist()),\n",
        "        'null_count': int(df[col].isna().sum()),\n",
        "        'unique_values': int(df[col].nunique()),\n",
        "    }\n",
        "\n",
        "    # Adicionar informações específicas para colunas importantes\n",
        "    if 'price' in col:\n",
        "        col_info['description'] = 'Preço do produto em unidade monetária (provavelmente USD)'\n",
        "    elif 'description' == col:\n",
        "        col_info['description'] = 'Descrição completa do produto'\n",
        "    elif 'name' in col:\n",
        "        col_info['description'] = 'Nome do produto'\n",
        "    elif 'dimension' in col:\n",
        "        col_info['description'] = 'Dimensões físicas do produto'\n",
        "    elif 'bullet_points' in col:\n",
        "        col_info['description'] = 'Características principais do produto em formato de lista'\n",
        "\n",
        "    data_dict.append(col_info)\n",
        "\n",
        "# Salvar dicionário de dados\n",
        "import json\n",
        "with open(f\"{standardized_zone}/data_dictionary_{timestamp}.json\", 'w') as f:\n",
        "    json.dump(data_dict, f, indent=2)\n",
        "\n",
        "log_operation(\"Processo de padronização concluído com sucesso\")\n",
        "\n",
        "# Exibir informações sobre o dataset padronizado\n",
        "print(\"\\nInformações do Dataset Padronizado:\")\n",
        "print(f\"Número de registros: {df.shape[0]}\")\n",
        "print(f\"Número de colunas: {df.shape[1]}\")\n",
        "print(\"\\nNovas colunas criadas:\")\n",
        "original_cols = set(pd.read_parquet(masked_file_path).columns)\n",
        "new_cols = set(df.columns) - original_cols\n",
        "print(list(new_cols))"
      ],
      "metadata": {
        "id": "YbBiEtIRk6yP",
        "outputId": "968502ae-01aa-4413-8041-a24b8e0e89d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-02-27 16:00:56.096281] Dataset carregado com sucesso. Dimensões: (2249698, 6)\n",
            "[2025-02-27 16:00:56.096492] Padronizando nomes das colunas\n",
            "[2025-02-27 16:00:56.096895] Iniciando limpeza e padronização dos dados\n",
            "[2025-02-27 16:00:56.235060] Limpando coluna de texto: title\n",
            "[2025-02-27 16:01:12.380797] Limpando coluna de texto: bullet_points\n",
            "[2025-02-27 16:01:54.106415] Limpando coluna de texto: description\n",
            "[2025-02-27 16:02:34.256469] Tratando valores nulos\n",
            "[2025-02-27 16:02:35.385383] Extraindo features de texto\n",
            "[2025-02-27 16:02:45.419108] Codificando variáveis categóricas\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'standardized_zone' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-368ba84ff3fb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m# 7. Salvar o dataset padronizado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d_%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mstandardized_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{standardized_zone}/amazon_products_standardized_{timestamp}.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandardized_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'standardized_zone' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardized Zone"
      ],
      "metadata": {
        "id": "kbXnPn4mmGzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_standardized = pd.read_csv('dataset/train.csv')\n",
        "\n",
        "# Handle missing values by reassigning to the columns\n",
        "df_standardized['TITLE'] = df_standardized['TITLE'].fillna('not provided')\n",
        "df_standardized['BULLET_POINTS'] = df_standardized['BULLET_POINTS'].fillna('not provided')\n",
        "df_standardized['DESCRIPTION'] = df_standardized['DESCRIPTION'].fillna('not provided')\n",
        "\n",
        "# Basic text cleaning for Description and Bullet Points (remove special characters)\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "        return text.lower().strip()\n",
        "    return ''\n",
        "\n",
        "# Apply text cleaning function\n",
        "df_standardized['DESCRIPTION'] = df_standardized['DESCRIPTION'].apply(clean_text)\n",
        "df_standardized['BULLET_POINTS'] = df_standardized['BULLET_POINTS'].apply(clean_text)\n",
        "\n",
        "# Display cleaned data\n",
        "print(\"Standardized Data:\")\n",
        "display(df_standardized.head())"
      ],
      "metadata": {
        "id": "6lUtRIsbmJr7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}